{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "                                              review  index\n",
      "0  One of the other reviewers has mentioned that ...      0\n",
      "1  A wonderful little production. <br /><br />The...      1\n",
      "2  I thought this was a wonderful way to spend ti...      2\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...      4\n",
      "5  Probably my all-time favorite movie, a story o...      5\n",
      "25000\n",
      "                                               review  index\n",
      "3   Basically there's a family where a little boy ...      3\n",
      "7   This show was an amazing, fresh & innovative i...      7\n",
      "8   Encouraged by the positive comments about this...      8\n",
      "10  Phil the Alien is one of those quirky films wh...     10\n",
      "11  I saw this movie when I was about 12 when it c...     11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('IMDB-Dataset.csv', error_bad_lines=False);\n",
    "\n",
    "# split positive and negative sentiment reviews\n",
    "pos_reviews = data[data.sentiment == \"positive\"]\n",
    "neg_reviews = data[data.sentiment == \"negative\"]\n",
    "\n",
    "pos_data = pos_reviews[['review']]\n",
    "pos_data['index'] = pos_data.index\n",
    "pos_documents = pos_data\n",
    "\n",
    "neg_data = neg_reviews[['review']]\n",
    "neg_data['index'] = neg_data.index\n",
    "neg_documents = neg_data\n",
    "\n",
    "# showing that the reviews were correctly split by sentiment\n",
    "print(len(pos_documents))\n",
    "print(pos_documents[:5])\n",
    "print(len(neg_documents))\n",
    "print(neg_documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "'''Positive Data'''\n",
    "# Remove punctuation using regular expresssion\n",
    "pos_documents['review_processed'] = pos_documents['review'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "# Lowercase the words using regulatr expresssion\n",
    "pos_documents['review_processed'] = pos_documents['review'].map(lambda x: x.lower())\n",
    "'''Negative Data'''\n",
    "# Remove punctuation using regular expresssion\n",
    "neg_documents['review_processed'] = neg_documents['review'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "# Lowercase the words using regulatr expresssion\n",
    "neg_documents['review_processed'] = neg_documents['review'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Cloud generated from positive reviews with minimal preprocessing:\n",
      "Word Cloud generated from negative reviews with minimal preprocessing:\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "print(\"Word Cloud generated from positive reviews with minimal preprocessing:\")\n",
    "long_string = \" \".join(pos_documents.review_processed)\n",
    "\n",
    "wordcloud = WordCloud().generate(long_string)\n",
    "image = wordcloud.to_image()\n",
    "image.show()\n",
    "\n",
    "print(\"Word Cloud generated from negative reviews with minimal preprocessing:\")\n",
    "long_string = \" \".join(neg_documents.review_processed)\n",
    "wordcloud = WordCloud().generate(long_string)\n",
    "image = wordcloud.to_image()\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive reviews after lemmatizing and stemming:\n",
      "25000\n",
      "0     [review, mention, watch, episod, hook, right, ...\n",
      "1     [wonder, littl, product, film, techniqu, unass...\n",
      "2     [think, wonder, spend, time, summer, weekend, ...\n",
      "4     [petter, mattei, love, time, money, visual, st...\n",
      "5     [probabl, time, favorit, movi, stori, selfless...\n",
      "6     [sure, like, resurrect, date, seahunt, seri, t...\n",
      "9     [like, origin, wrench, laughter, like, movi, y...\n",
      "14    [fantast, movi, prison, famous, actor, georg, ...\n",
      "16    [film, simpli, remak, film, fail, captur, flav...\n",
      "18    [rememb, film, film, watch, cinema, pictur, da...\n",
      "Name: review_processed, dtype: object\n",
      "\n",
      "Negative reviews after lemmatizing and stemming:\n",
      "25000\n",
      "3     [basic, famili, littl, jake, think, zombi, clo...\n",
      "7     [amaz, fresh, innov, idea, air, year, brillian...\n",
      "8     [encourag, posit, comment, film, look, forward...\n",
      "10    [phil, alien, quirki, film, humour, base, odd,...\n",
      "11    [movi, come, recal, scariest, scene, bird, eat...\n",
      "12    [boll, work, enjoy, movi, postal, mayb, boll, ...\n",
      "13    [cast, play, shakespear, shakespear, lose, app...\n",
      "15    [kind, draw, erot, scene, realiz, amateurish, ...\n",
      "17    [movi, aw, movi, horribl, wasn, continu, minut...\n",
      "19    [aw, film, real, stinker, nomin, golden, globe...\n",
      "Name: review_processed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pos_processed_documents = pos_documents['review_processed'].map(preprocess)\n",
    "print(\"Positive reviews after lemmatizing and stemming:\")\n",
    "print(len(pos_processed_documents))\n",
    "print(pos_processed_documents[:10])\n",
    "\n",
    "neg_processed_documents = neg_documents['review_processed'].map(preprocess)\n",
    "print(\"\\nNegative reviews after lemmatizing and stemming:\")\n",
    "print(len(neg_processed_documents))\n",
    "print(neg_processed_documents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 accustom\n",
      "1 agenda\n",
      "2 agreement\n",
      "3 appeal\n",
      "4 aryan\n",
      "5 audienc\n",
      "6 away\n",
      "7 bitch\n",
      "8 brutal\n",
      "9 call\n",
      "10 cell\n",
      "0 argu\n",
      "1 basic\n",
      "2 boogeyman\n",
      "3 closet\n",
      "4 decid\n",
      "5 descent\n",
      "6 dialog\n",
      "7 divorc\n",
      "8 drama\n",
      "9 expect\n",
      "10 famili\n"
     ]
    }
   ],
   "source": [
    "pos_dictionary = gensim.corpora.Dictionary(pos_processed_documents)\n",
    "count = 0\n",
    "for k, v in pos_dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "        \n",
    "neg_dictionary = gensim.corpora.Dictionary(neg_processed_documents)\n",
    "count = 0\n",
    "for k, v in neg_dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "neg_dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "pos_bow_corpus = [pos_dictionary.doc2bow(doc) for doc in neg_processed_documents]\n",
    "neg_bow_corpus = [neg_dictionary.doc2bow(doc) for doc in pos_processed_documents]\n",
    "pos_lda_model = gensim.models.LdaMulticore(pos_bow_corpus, num_topics=10, id2word=pos_dictionary, passes=2, workers=2)\n",
    "neg_lda_model = gensim.models.LdaMulticore(neg_bow_corpus, num_topics=10, id2word=neg_dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten Random Positive Review Topics:\n",
      "Topic: 0 \n",
      "Words: 0.015*\"like\" + 0.010*\"charact\" + 0.009*\"think\" + 0.008*\"good\" + 0.007*\"stori\" + 0.006*\"actor\" + 0.006*\"watch\" + 0.006*\"look\" + 0.005*\"come\" + 0.005*\"time\"\n",
      "Topic: 1 \n",
      "Words: 0.013*\"like\" + 0.012*\"time\" + 0.011*\"good\" + 0.011*\"watch\" + 0.010*\"act\" + 0.010*\"scene\" + 0.008*\"charact\" + 0.008*\"actor\" + 0.007*\"look\" + 0.006*\"think\"\n",
      "Topic: 2 \n",
      "Words: 0.011*\"charact\" + 0.009*\"like\" + 0.009*\"stori\" + 0.006*\"time\" + 0.005*\"scene\" + 0.005*\"good\" + 0.005*\"play\" + 0.004*\"work\" + 0.004*\"director\" + 0.004*\"look\"\n",
      "Topic: 3 \n",
      "Words: 0.013*\"good\" + 0.011*\"see\" + 0.010*\"stori\" + 0.009*\"watch\" + 0.009*\"act\" + 0.008*\"like\" + 0.006*\"plot\" + 0.006*\"better\" + 0.006*\"time\" + 0.006*\"worst\"\n",
      "Topic: 4 \n",
      "Words: 0.009*\"look\" + 0.008*\"horror\" + 0.008*\"like\" + 0.007*\"scene\" + 0.006*\"kill\" + 0.005*\"get\" + 0.005*\"time\" + 0.005*\"thing\" + 0.005*\"good\" + 0.005*\"girl\"\n",
      "Topic: 5 \n",
      "Words: 0.018*\"like\" + 0.012*\"peopl\" + 0.006*\"think\" + 0.006*\"time\" + 0.006*\"look\" + 0.005*\"play\" + 0.005*\"watch\" + 0.005*\"go\" + 0.005*\"black\" + 0.004*\"know\"\n",
      "Topic: 6 \n",
      "Words: 0.018*\"watch\" + 0.017*\"like\" + 0.012*\"think\" + 0.011*\"time\" + 0.009*\"peopl\" + 0.008*\"know\" + 0.008*\"see\" + 0.007*\"good\" + 0.007*\"worst\" + 0.007*\"thing\"\n",
      "Topic: 7 \n",
      "Words: 0.009*\"like\" + 0.009*\"charact\" + 0.006*\"watch\" + 0.006*\"know\" + 0.006*\"time\" + 0.005*\"good\" + 0.005*\"thing\" + 0.005*\"go\" + 0.005*\"think\" + 0.004*\"peopl\"\n",
      "Topic: 8 \n",
      "Words: 0.013*\"stori\" + 0.013*\"good\" + 0.011*\"origin\" + 0.010*\"time\" + 0.009*\"charact\" + 0.008*\"watch\" + 0.008*\"think\" + 0.008*\"like\" + 0.007*\"scene\" + 0.006*\"look\"\n",
      "Topic: 9 \n",
      "Words: 0.010*\"like\" + 0.009*\"know\" + 0.008*\"go\" + 0.008*\"watch\" + 0.007*\"time\" + 0.007*\"good\" + 0.006*\"scene\" + 0.005*\"act\" + 0.005*\"start\" + 0.005*\"peopl\"\n",
      "\n",
      "Ten Random Negative Review Topics\n",
      "Topic: 0 \n",
      "Words: 0.010*\"great\" + 0.009*\"play\" + 0.009*\"best\" + 0.008*\"good\" + 0.007*\"time\" + 0.007*\"love\" + 0.006*\"stori\" + 0.006*\"actor\" + 0.006*\"music\" + 0.005*\"year\"\n",
      "Topic: 1 \n",
      "Words: 0.009*\"charact\" + 0.009*\"stori\" + 0.007*\"time\" + 0.005*\"scene\" + 0.005*\"watch\" + 0.005*\"perform\" + 0.005*\"work\" + 0.004*\"act\" + 0.004*\"good\" + 0.004*\"plot\"\n",
      "Topic: 2 \n",
      "Words: 0.014*\"love\" + 0.013*\"charact\" + 0.010*\"stori\" + 0.010*\"life\" + 0.009*\"time\" + 0.007*\"great\" + 0.006*\"famili\" + 0.005*\"come\" + 0.005*\"live\" + 0.005*\"perform\"\n",
      "Topic: 3 \n",
      "Words: 0.009*\"love\" + 0.009*\"time\" + 0.008*\"year\" + 0.007*\"watch\" + 0.007*\"danc\" + 0.007*\"play\" + 0.007*\"match\" + 0.007*\"girl\" + 0.006*\"stori\" + 0.005*\"good\"\n",
      "Topic: 4 \n",
      "Words: 0.015*\"watch\" + 0.014*\"time\" + 0.010*\"think\" + 0.009*\"see\" + 0.009*\"love\" + 0.008*\"great\" + 0.008*\"know\" + 0.008*\"stori\" + 0.008*\"good\" + 0.007*\"music\"\n",
      "Topic: 5 \n",
      "Words: 0.012*\"think\" + 0.010*\"peopl\" + 0.009*\"watch\" + 0.008*\"time\" + 0.007*\"show\" + 0.007*\"stori\" + 0.006*\"love\" + 0.006*\"good\" + 0.006*\"see\" + 0.006*\"know\"\n",
      "Topic: 6 \n",
      "Words: 0.007*\"scene\" + 0.006*\"stori\" + 0.005*\"time\" + 0.005*\"charact\" + 0.004*\"work\" + 0.004*\"director\" + 0.004*\"great\" + 0.003*\"play\" + 0.003*\"beauti\" + 0.003*\"see\"\n",
      "Topic: 7 \n",
      "Words: 0.011*\"seri\" + 0.007*\"time\" + 0.007*\"episod\" + 0.006*\"good\" + 0.005*\"stori\" + 0.005*\"play\" + 0.004*\"charact\" + 0.004*\"anim\" + 0.004*\"look\" + 0.003*\"go\"\n",
      "Topic: 8 \n",
      "Words: 0.008*\"good\" + 0.006*\"play\" + 0.005*\"stori\" + 0.005*\"look\" + 0.005*\"go\" + 0.004*\"get\" + 0.004*\"time\" + 0.004*\"scene\" + 0.004*\"want\" + 0.004*\"thing\"\n",
      "Topic: 9 \n",
      "Words: 0.023*\"good\" + 0.015*\"great\" + 0.013*\"think\" + 0.011*\"charact\" + 0.009*\"watch\" + 0.009*\"scene\" + 0.007*\"time\" + 0.007*\"see\" + 0.006*\"thing\" + 0.006*\"look\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Ten Random Positive Review Topics:\")\n",
    "for idx, topic in pos_lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    \n",
    "print(\"\\nTen Random Negative Review Topics\")\n",
    "for idx, topic in neg_lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#extracting positive topic words\n",
    "x = pos_lda_model.show_topics(num_topics=12, num_words=900,formatted=False)\n",
    "topics = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "topics = [x[1] for x in topics]\n",
    "pos_topic_words = []\n",
    "for words in topics:\n",
    "    for word in words:\n",
    "        pos_topic_words.append(word)\n",
    "        \n",
    "#extracting negative topic words\n",
    "x = neg_lda_model.show_topics(num_topics=12, num_words=900,formatted=False)\n",
    "topics = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "topics = [x[1] for x in topics]\n",
    "neg_topic_words = []\n",
    "for words in topics:\n",
    "    for word in words:\n",
    "        neg_topic_words.append(word)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Topics: 9000\n",
      "['like', 'charact', 'think', 'good', 'stori']\n",
      "\n",
      "Negative Topics: 9000\n",
      "['great', 'play', 'best', 'good', 'time']\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation using regular expresssion\n",
    "pos_topic_words = [re.sub('[,\\.!?]', '', word) for word in pos_topic_words]\n",
    "neg_topic_words = [re.sub('[,\\.!?]', '', word) for word in neg_topic_words]\n",
    "# showing that there are now root words in our lists\n",
    "print('Positive Topics: {}'.format(len(pos_topic_words)))\n",
    "print(pos_topic_words[:5])\n",
    "print('\\nNegative Topics: {}'.format(len(neg_topic_words)))\n",
    "print(neg_topic_words[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
