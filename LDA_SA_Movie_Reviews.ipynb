{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained models or train from scratch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickled files/models? (y/n, defaults to y): y\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "use_pickled = input(\"Load pickled files/models? (y/n, defaults to y): \")\n",
    "if use_pickled == 'n':\n",
    "    use_pickled = False\n",
    "else:\n",
    "    use_pickled = True\n",
    "\n",
    "pickled_dir = \"pickled\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the IMDB movie review dataset\n",
    "This dataset is already separated into 25,000 negative and 25,000 positive reviews for a total of 50,000 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "                                              review  index\n",
      "0  One of the other reviewers has mentioned that ...      0\n",
      "1  A wonderful little production. <br /><br />The...      1\n",
      "2  I thought this was a wonderful way to spend ti...      2\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...      4\n",
      "5  Probably my all-time favorite movie, a story o...      5\n",
      "25000\n",
      "                                               review  index\n",
      "3   Basically there's a family where a little boy ...      3\n",
      "7   This show was an amazing, fresh & innovative i...      7\n",
      "8   Encouraged by the positive comments about this...      8\n",
      "10  Phil the Alien is one of those quirky films wh...     10\n",
      "11  I saw this movie when I was about 12 when it c...     11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('IMDB-Dataset.csv', error_bad_lines=False);\n",
    "\n",
    "# split positive and negative sentiment reviews\n",
    "pos_reviews = data[data.sentiment == \"positive\"]\n",
    "neg_reviews = data[data.sentiment == \"negative\"]\n",
    "\n",
    "pos_data = pos_reviews[['review']]\n",
    "pos_data['index'] = pos_data.index\n",
    "pos_documents = pos_data\n",
    "\n",
    "neg_data = neg_reviews[['review']]\n",
    "neg_data['index'] = neg_data.index\n",
    "neg_documents = neg_data\n",
    "\n",
    "# showing that the reviews were correctly split by sentiment\n",
    "print(len(pos_documents))\n",
    "print(pos_documents[:5])\n",
    "print(len(neg_documents))\n",
    "print(neg_documents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Preprocessing: remove punctuation and convert everything to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "'''Positive Data'''\n",
    "# Remove punctuation using regular expresssion\n",
    "pos_documents['review_processed'] = pos_documents['review'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "# Lowercase the words using regular expresssion\n",
    "pos_documents['review_processed'] = pos_documents['review'].map(lambda x: x.lower())\n",
    "'''Negative Data'''\n",
    "# Remove punctuation using regular expresssion\n",
    "neg_documents['review_processed'] = neg_documents['review'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "# Lowercase the words using regular expresssion\n",
    "neg_documents['review_processed'] = neg_documents['review'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate World Cloud\n",
    "\n",
    "Either loads the pickled objects from disk, or genereate a new word cloud objects and save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# load from pickled/wordcloud_pos and pickled/worldcloud_neg\n",
    "if use_pickled:\n",
    "    with open(os.path.join(pickled_dir, \"wordcloud_pos\"), 'rb') as f:\n",
    "        wordcloud_pos = pickle.load(f)\n",
    "    with open(os.path.join(pickled_dir, \"wordcloud_neg\"), 'rb') as f:\n",
    "        wordcloud_neg = pickle.load(f)\n",
    "else:\n",
    "    # create a new wordcloud object for positive reviews, then write it to disk\n",
    "    long_string_pos = \" \".join(pos_documents.review_processed)\n",
    "    wordcloud_pos = WordCloud().generate(long_string_pos)\n",
    "    with open(os.path.join(pickled_dir, \"wordcloud_pos\"), 'wb+') as f:\n",
    "        pickle.dump(wordcloud_pos, f)\n",
    "    \n",
    "    # create a new wordcloud object for negative reviews, then write it to disk\n",
    "    long_string_neg = \" \".join(neg_documents.review_processed)\n",
    "    wordcloud_neg = WordCloud().generate(long_string_neg)\n",
    "    with open(os.path.join(pickled_dir, \"wordcloud_neg\"), 'wb+') as f:\n",
    "        pickle.dump(wordcloud_neg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postitive Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = wordcloud_pos.to_image()\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = wordcloud_neg.to_image()\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lematizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive reviews after lemmatizing and stemming:\n",
      "25000\n",
      "0     [review, mention, watch, episod, hook, right, ...\n",
      "1     [wonder, littl, product, film, techniqu, unass...\n",
      "2     [think, wonder, spend, time, summer, weekend, ...\n",
      "4     [petter, mattei, love, time, money, visual, st...\n",
      "5     [probabl, time, favorit, movi, stori, selfless...\n",
      "6     [sure, like, resurrect, date, seahunt, seri, t...\n",
      "9     [like, origin, wrench, laughter, like, movi, y...\n",
      "14    [fantast, movi, prison, famous, actor, georg, ...\n",
      "16    [film, simpli, remak, film, fail, captur, flav...\n",
      "18    [rememb, film, film, watch, cinema, pictur, da...\n",
      "Name: review_processed, dtype: object\n",
      "\n",
      "Negative reviews after lemmatizing and stemming:\n",
      "25000\n",
      "3     [basic, famili, littl, jake, think, zombi, clo...\n",
      "7     [amaz, fresh, innov, idea, air, year, brillian...\n",
      "8     [encourag, posit, comment, film, look, forward...\n",
      "10    [phil, alien, quirki, film, humour, base, odd,...\n",
      "11    [movi, come, recal, scariest, scene, bird, eat...\n",
      "12    [boll, work, enjoy, movi, postal, mayb, boll, ...\n",
      "13    [cast, play, shakespear, shakespear, lose, app...\n",
      "15    [kind, draw, erot, scene, realiz, amateurish, ...\n",
      "17    [movi, aw, movi, horribl, wasn, continu, minut...\n",
      "19    [aw, film, real, stinker, nomin, golden, globe...\n",
      "Name: review_processed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "pos_proc_docs_fname = os.path.join(pickled_dir, \"pos_processed_docs\")\n",
    "neg_proc_docs_fname = os.path.join(pickled_dir, \"neg_processed_docs\")\n",
    "if use_pickled:\n",
    "    with open(pos_proc_docs_fname, 'rb') as f1:\n",
    "        pos_processed_documents = pickle.load(f1)\n",
    "    with open(neg_proc_docs_fname, 'rb') as f2:\n",
    "        neg_processed_documents = pickle.load(f2)\n",
    "else:\n",
    "    pos_processed_documents = pos_documents['review_processed'].map(preprocess)\n",
    "    neg_processed_documents = neg_documents['review_processed'].map(preprocess)\n",
    "    with open(pos_proc_docs_fname, 'wb+') as f1:\n",
    "        pickle.dump(pos_processed_documents, f1)\n",
    "    with open(neg_proc_docs_fname, 'wb+') as f2:\n",
    "        pickle.dump(neg_processed_documents, f2)\n",
    "\n",
    "print(\"Positive reviews after lemmatizing and stemming:\")\n",
    "print(len(pos_processed_documents))\n",
    "print(pos_processed_documents[:10])\n",
    "\n",
    "print(\"\\nNegative reviews after lemmatizing and stemming:\")\n",
    "print(len(neg_processed_documents))\n",
    "print(neg_processed_documents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some random positive words in our dictionary: \n",
      "0 accustom\n",
      "1 agenda\n",
      "2 agreement\n",
      "3 appeal\n",
      "4 aryan\n",
      "5 audienc\n",
      "6 away\n",
      "\n",
      "Some random negative words in our dictionary: \n",
      "0 argu\n",
      "1 basic\n",
      "2 boogeyman\n",
      "3 closet\n",
      "4 decid\n",
      "5 descent\n",
      "6 dialog\n"
     ]
    }
   ],
   "source": [
    "# Making Positive and Negative Dictionaries\n",
    "pos_dictionary = gensim.corpora.Dictionary(pos_processed_documents)\n",
    "count = 0\n",
    "print(\"\\nSome random positive words in our dictionary: \")\n",
    "for k, v in pos_dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 6:\n",
    "        break    \n",
    "neg_dictionary = gensim.corpora.Dictionary(neg_processed_documents)\n",
    "count = 0\n",
    "print(\"\\nSome random negative words in our dictionary: \")\n",
    "for k, v in neg_dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Extreme Cases of Words\n",
    "pos_dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "pos_bow_corpus = [pos_dictionary.doc2bow(doc) for doc in pos_processed_documents] # corpus for topics that are seen as positive\n",
    "\n",
    "neg_dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "neg_bow_corpus = [neg_dictionary.doc2bow(doc) for doc in neg_processed_documents] # corpus for topics that are seen as negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the LDA Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lda_model_fname = os.path.join(pickled_dir, 'pos_lda_model')\n",
    "neg_lda_model_fname = os.path.join(pickled_dir, 'neg_lda_model')\n",
    "if use_pickled:\n",
    "    pos_lda_model = gensim.models.LdaMulticore.load(pos_lda_model_fname)\n",
    "    neg_lda_model = gensim.models.LdaMulticore.load(neg_lda_model_fname)\n",
    "\n",
    "else:\n",
    "    pos_lda_model = gensim.models.LdaMulticore(\n",
    "        pos_bow_corpus, \n",
    "        num_topics=150, \n",
    "        id2word=pos_dictionary, \n",
    "        passes=10, \n",
    "        workers=6)\n",
    "\n",
    "    neg_lda_model = gensim.models.LdaMulticore(\n",
    "        neg_bow_corpus, \n",
    "        num_topics=150, \n",
    "        id2word=neg_dictionary, \n",
    "        passes=10, \n",
    "        workers=6)\n",
    "    \n",
    "    pos_lda_model.save(pos_lda_model_fname)\n",
    "    neg_lda_model.save(neg_lda_model_fname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Ten Positive Review Topics:\n",
      "Topic: 0 \n",
      "Words: 0.016*\"charact\" + 0.015*\"like\" + 0.015*\"great\" + 0.014*\"good\" + 0.013*\"actor\" + 0.009*\"stori\" + 0.008*\"see\" + 0.008*\"think\" + 0.008*\"watch\" + 0.007*\"play\"\n",
      "Topic: 1 \n",
      "Words: 0.017*\"scene\" + 0.017*\"watch\" + 0.017*\"time\" + 0.013*\"good\" + 0.012*\"great\" + 0.011*\"enjoy\" + 0.010*\"love\" + 0.010*\"chaplin\" + 0.010*\"comedi\" + 0.009*\"funni\"\n",
      "Topic: 2 \n",
      "Words: 0.012*\"charact\" + 0.009*\"play\" + 0.008*\"stori\" + 0.007*\"scene\" + 0.006*\"perform\" + 0.005*\"role\" + 0.005*\"director\" + 0.005*\"time\" + 0.005*\"great\" + 0.004*\"like\"\n",
      "Topic: 3 \n",
      "Words: 0.022*\"draw\" + 0.015*\"watch\" + 0.011*\"time\" + 0.011*\"like\" + 0.010*\"play\" + 0.010*\"good\" + 0.009*\"nanci\" + 0.009*\"wonder\" + 0.009*\"enjoy\" + 0.008*\"know\"\n",
      "Topic: 4 \n",
      "Words: 0.020*\"girl\" + 0.013*\"silli\" + 0.012*\"good\" + 0.011*\"enjoy\" + 0.011*\"play\" + 0.010*\"like\" + 0.010*\"villain\" + 0.010*\"thing\" + 0.010*\"peopl\" + 0.010*\"great\"\n",
      "Topic: 5 \n",
      "Words: 0.018*\"ring\" + 0.014*\"stori\" + 0.009*\"lord\" + 0.009*\"love\" + 0.008*\"emili\" + 0.008*\"anim\" + 0.008*\"power\" + 0.008*\"year\" + 0.008*\"charact\" + 0.007*\"martha\"\n",
      "Topic: 6 \n",
      "Words: 0.074*\"episod\" + 0.025*\"seri\" + 0.013*\"best\" + 0.010*\"season\" + 0.009*\"charact\" + 0.008*\"time\" + 0.008*\"final\" + 0.008*\"plot\" + 0.008*\"cast\" + 0.008*\"watch\"\n",
      "Topic: 7 \n",
      "Words: 0.019*\"action\" + 0.018*\"andi\" + 0.015*\"hank\" + 0.014*\"love\" + 0.013*\"carlo\" + 0.012*\"brother\" + 0.011*\"famili\" + 0.010*\"terrorist\" + 0.010*\"money\" + 0.010*\"good\"\n",
      "Topic: 8 \n",
      "Words: 0.019*\"page\" + 0.012*\"paint\" + 0.011*\"good\" + 0.009*\"watch\" + 0.008*\"stori\" + 0.008*\"minut\" + 0.007*\"scene\" + 0.006*\"complet\" + 0.006*\"want\" + 0.006*\"pick\"\n",
      "Topic: 9 \n",
      "Words: 0.014*\"good\" + 0.010*\"davi\" + 0.010*\"play\" + 0.008*\"powel\" + 0.008*\"time\" + 0.008*\"like\" + 0.007*\"perform\" + 0.007*\"case\" + 0.007*\"stori\" + 0.007*\"love\"\n",
      "\n",
      "First Ten Negative Review Topics\n",
      "Topic: 0 \n",
      "Words: 0.020*\"littl\" + 0.018*\"stori\" + 0.014*\"wave\" + 0.011*\"chuck\" + 0.011*\"peopl\" + 0.011*\"wasn\" + 0.011*\"scene\" + 0.010*\"tell\" + 0.009*\"time\" + 0.009*\"willi\"\n",
      "Topic: 1 \n",
      "Words: 0.015*\"ninja\" + 0.014*\"martin\" + 0.012*\"cole\" + 0.012*\"steve\" + 0.011*\"time\" + 0.010*\"wear\" + 0.010*\"stori\" + 0.008*\"battl\" + 0.008*\"fight\" + 0.007*\"look\"\n",
      "Topic: 2 \n",
      "Words: 0.013*\"charact\" + 0.012*\"scene\" + 0.011*\"watch\" + 0.010*\"moron\" + 0.008*\"littl\" + 0.007*\"peopl\" + 0.007*\"say\" + 0.007*\"plot\" + 0.007*\"come\" + 0.007*\"thing\"\n",
      "Topic: 3 \n",
      "Words: 0.013*\"time\" + 0.012*\"giant\" + 0.011*\"monster\" + 0.010*\"gold\" + 0.010*\"good\" + 0.010*\"river\" + 0.008*\"color\" + 0.008*\"stori\" + 0.007*\"plot\" + 0.006*\"scene\"\n",
      "Topic: 4 \n",
      "Words: 0.022*\"charli\" + 0.018*\"action\" + 0.016*\"agent\" + 0.012*\"stephen\" + 0.012*\"baldwin\" + 0.012*\"terrorist\" + 0.010*\"peopl\" + 0.010*\"fact\" + 0.009*\"script\" + 0.009*\"take\"\n",
      "Topic: 5 \n",
      "Words: 0.020*\"idea\" + 0.018*\"good\" + 0.015*\"act\" + 0.013*\"stage\" + 0.012*\"music\" + 0.011*\"write\" + 0.010*\"better\" + 0.010*\"see\" + 0.010*\"star\" + 0.010*\"direct\"\n",
      "Topic: 6 \n",
      "Words: 0.038*\"human\" + 0.019*\"scienc\" + 0.017*\"planet\" + 0.014*\"plot\" + 0.013*\"fiction\" + 0.011*\"ape\" + 0.011*\"earth\" + 0.009*\"good\" + 0.008*\"origin\" + 0.008*\"thing\"\n",
      "Topic: 7 \n",
      "Words: 0.023*\"watch\" + 0.019*\"good\" + 0.014*\"theatr\" + 0.012*\"murphi\" + 0.011*\"charact\" + 0.010*\"bore\" + 0.010*\"point\" + 0.010*\"angel\" + 0.009*\"act\" + 0.009*\"year\"\n",
      "Topic: 8 \n",
      "Words: 0.024*\"watch\" + 0.022*\"good\" + 0.021*\"lame\" + 0.019*\"act\" + 0.017*\"freddi\" + 0.017*\"time\" + 0.014*\"nightmar\" + 0.014*\"minut\" + 0.013*\"perform\" + 0.013*\"band\"\n",
      "Topic: 9 \n",
      "Words: 0.015*\"interview\" + 0.010*\"thunderbird\" + 0.010*\"documentari\" + 0.009*\"watch\" + 0.009*\"hood\" + 0.008*\"punk\" + 0.007*\"time\" + 0.007*\"duke\" + 0.007*\"john\" + 0.007*\"go\"\n"
     ]
    }
   ],
   "source": [
    "# Viewing the LDA Model Topic Results\n",
    "print(\"\\nFirst Ten Positive Review Topics:\")\n",
    "for idx, topic in pos_lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    if idx>=9:\n",
    "        break\n",
    "print(\"\\nFirst Ten Negative Review Topics\")\n",
    "for idx, topic in neg_lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    if idx>=9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity: An evaluation of how good the models are. Lower is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# compute perplexity, a measure of how good the model is. lower the better.\n",
    "print('\\nPositive LDA Model Perplexity: ', pos_lda_model.log_perplexity(pos_bow_corpus,))\n",
    "print('\\nNegative LDA Model Perplexity: ', neg_lda_model.log_perplexity(neg_bow_corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "pos_coherence_model_lda = CoherenceModel(model=pos_lda_model,texts=pos_processed_documents, dictionary=pos_dictionary, coherence='c_v')\n",
    "pos_coherence_lda = pos_coherence_model_lda.get_coherence()\n",
    "print('Positive LDA Model Coherence Score: ', pos_coherence_lda)\n",
    "\n",
    "# Compute Coherence Score\n",
    "neg_coherence_model_lda = CoherenceModel(model=neg_lda_model, texts=neg_processed_documents, dictionary=neg_dictionary, coherence='c_v')\n",
    "neg_coherence_lda = neg_coherence_model_lda.get_coherence()\n",
    "print('Negative LDA Model Coherence Score: ', neg_coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim\n",
    "\n",
    "#'''Positive'''\n",
    "# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "#mallet_path = r'C:/MALLET/bin/mallet.bat' # update this path\n",
    "#pos_ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=pos_bow_corpus, num_topics=30, id2word=pos_dictionary)\n",
    "\n",
    "# Show Topics\n",
    "#print(pos_ldamallet.show_topics(formatted=False))\n",
    "# Compute Coherence Score\n",
    "#pos_coherence_model_ldamallet = CoherenceModel(model=pos_ldamallet, texts=pos_processed, dictionary=pos_dictionary, coherence='c_v')\n",
    "#pos_coherence_ldamallet = pos_coherence_model_ldamallet.get_coherence()\n",
    "#print('\\nCoherence Score: ', pos_coherence_ldamallet)\n",
    "\n",
    "#'''Negative'''\n",
    "#neg_ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=neg_bow_corpus, num_topics=30, id2word=neg_dictionary)\n",
    "\n",
    "# Show Topics\n",
    "#print(neg_ldamallet.show_topics(formatted=False))\n",
    "# Compute Coherence Score\n",
    "#neg_coherence_model_ldamallet = CoherenceModel(model=neg_ldamallet, texts=neg_processed, dictionary=neg_dictionary, coherence='c_v')\n",
    "#neg_coherence_ldamallet = neg_coherence_model_ldamallet.get_coherence()\n",
    "#print('\\nCoherence Score: ', neg_coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# Visualize positive topic words\n",
    "pyLDAvis.enable_notebook()\n",
    "pos_vis = pyLDAvis.gensim.prepare(pos_lda_model, pos_bow_corpus, pos_dictionary)\n",
    "pos_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize negative topic words\n",
    "pyLDAvis.enable_notebook()\n",
    "neg_vis = pyLDAvis.gensim.prepare(neg_lda_model, neg_bow_corpus, neg_dictionary)\n",
    "neg_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    count = 0\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    #get what positive topics might be related\n",
    "    bow_vector = pos_dictionary.doc2bow(preprocess(text))\n",
    "    \n",
    "    for idx, score in sorted(pos_lda_model[bow_vector], key=lambda tup:-1*tup[1]):\n",
    "        count+=1\n",
    "        pos_score += score\n",
    "        if count > 2:\n",
    "            break\n",
    "    pos_score = pos_score/3\n",
    "    pos_score *= 100\n",
    "    \n",
    "    count = 0\n",
    "    #get what negative topics might be related\n",
    "    bow_vector = neg_dictionary.doc2bow(preprocess(text))\n",
    "    for idx, score in sorted(neg_lda_model[bow_vector], key=lambda tup:-1*tup[1]):    \n",
    "        neg_score += score\n",
    "        count+=1\n",
    "        if count > 2:\n",
    "            break\n",
    "    neg_score = neg_score/3\n",
    "    neg_score *= 100\n",
    "    \n",
    "    result = 0\n",
    "    if pos_score>neg_score:\n",
    "        result = (pos_score - neg_score)/(pos_score + neg_score)\n",
    "        result *= 100\n",
    "        result = 100 - (result*2)\n",
    "        return result\n",
    "    else:\n",
    "        result = (neg_score - pos_score)/(pos_score + neg_score)\n",
    "        result *= 100\n",
    "        result = 100 - (result*2)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_general_sentiment(text):\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    #get what positive topics might be related\n",
    "    bow_vector = pos_dictionary.doc2bow(preprocess(text))\n",
    "    \n",
    "    for idx, score in sorted(pos_lda_model[bow_vector], key=lambda tup:-1*tup[1]):\n",
    "        pos_score += score\n",
    "        break;\n",
    "   \n",
    "    #get what negative topics might be related\n",
    "    bow_vector = neg_dictionary.doc2bow(preprocess(text))\n",
    "    for idx, score in sorted(neg_lda_model[bow_vector], key=lambda tup:-1*tup[1]):    \n",
    "        neg_score += score\n",
    "        break;\n",
    "\n",
    "    if pos_score>neg_score:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a movie description to analyze: A young Bruce Wayne (Christian Bale) travels to the Far East, where he's trained in the martial arts by Henri Ducard (Liam Neeson), a member of the mysterious League of Shadows. When Ducard reveals the League's true purpose -- the complete destruction of Gotham City -- Wayne returns to Gotham intent on cleaning up the city without resorting to murder. With the help of Alfred (Michael Caine), his loyal butler, and Lucius Fox (Morgan Freeman), a tech expert at Wayne Enterprises, Batman is born.\n",
      "\n",
      "We predict that opinions on this movie are generally positive.\n",
      "\n",
      "We predict that this movie has a rating of ~98.044%.\n"
     ]
    }
   ],
   "source": [
    "unseen_movie_description = input(\"Please enter a movie description to analyze: \")\n",
    "result = round(get_sentiment(unseen_movie_description), 3)\n",
    "print(\"\\nWe predict that opinions on this movie are generally {}.\".format(get_general_sentiment(unseen_movie_description)))\n",
    "print(\"\\nWe predict that this movie has a rating of ~{}%.\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
